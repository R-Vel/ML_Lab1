{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e226baf-fe4c-430b-b357-98eab66633d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T03:39:50.782619Z",
     "iopub.status.busy": "2025-09-29T03:39:50.780612Z",
     "iopub.status.idle": "2025-09-29T03:39:51.190576Z",
     "shell.execute_reply": "2025-09-29T03:39:51.181630Z",
     "shell.execute_reply.started": "2025-09-29T03:39:50.782409Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js \"></script><script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       "if (code_show){\n",
       "$('div.jp-CodeCell > div.jp-Cell-inputWrapper').hide();\n",
       "} else {\n",
       "$('div.jp-CodeCell > div.jp-Cell-inputWrapper').show();\n",
       "}\n",
       "code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);</script><form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js \"></script><script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    "if (code_show){\n",
    "$('div.jp-CodeCell > div.jp-Cell-inputWrapper').hide();\n",
    "} else {\n",
    "$('div.jp-CodeCell > div.jp-Cell-inputWrapper').show();\n",
    "}\n",
    "code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);</script><form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3e371bed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T03:36:19.675845Z",
     "iopub.status.busy": "2025-09-30T03:36:19.673517Z",
     "iopub.status.idle": "2025-09-30T03:36:20.007392Z",
     "shell.execute_reply": "2025-09-30T03:36:20.000764Z",
     "shell.execute_reply.started": "2025-09-30T03:36:19.675590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    average_precision_score,\n",
    "    make_scorer\n",
    ")\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from imblearn.under_sampling import TomekLinks, RandomUnderSampler\n",
    "\n",
    "from model import FraudDetector\n",
    "\n",
    "# Notebook autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# For reproducibility\n",
    "RANDOM_STATE = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "25f15d1b-35f2-4f6b-83f1-8a22b89fcdf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T01:50:01.273667Z",
     "iopub.status.busy": "2025-09-29T01:50:01.271518Z",
     "iopub.status.idle": "2025-09-29T01:50:25.100533Z",
     "shell.execute_reply": "2025-09-29T01:50:25.094093Z",
     "shell.execute_reply.started": "2025-09-29T01:50:01.273502Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"historical.csv\")\n",
    "target = data[\"outcome\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228657d-92f3-401d-a18b-da1858286a8c",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background: url('https://imgur.com/gtF7pEr.png') no-repeat center center; \n",
    "    background-size: cover;\n",
    "    height: 1000px;\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "    justify-content: center;\n",
    "    align-items: center;\n",
    "    text-align: center;\n",
    "    color: white;\n",
    "    padding: 20px;\n",
    "\">\n",
    "    <h1 style=\"font-size: 50px; font-weight: bold; margin: 10px;\"></h1>\n",
    "    <h2 style=\"font-size: 30px; font-style: italic; margin: 10px;\"></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b94b488-56e2-4937-afb7-d5c6441dea0a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #215880; padding: 10px; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #e6f1fa; font-size: 30px; font-weight: bold; margin: 10px;\">ABSTRACT</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3870d435-8cbe-4eb7-940d-11e299c5b59b",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Poppins', sans-serif; font-size: 15px; border-bottom: 2px solid #2a475e; padding-bottom: 15px\">\n",
    "<p style=\"line-height: 1.5; text-align: justify\">\n",
    "aaaaaaaaa\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"border-bottom: 1px solid #b6c6de; box-shadow: 0px 1px 0px #66c0f4; padding-bottom: 7px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab84be-88fb-4224-9d1b-0d71fb0d4555",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"background-color: #215880; padding: 10px; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #e6f1fa; font-size: 30px; font-weight: bold; margin: 10px;\">INTRODUCTION</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe60b479-fa09-4992-b9f4-07d5f1df5951",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"background-color: #215880; padding: 10px; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #e6f1fa; font-size: 30px; font-weight: bold; margin: 10px;\">METHODOLOGY</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b208c45-0756-4d00-901b-e42d6ffbca00",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://imgur.com/5MtrMTE.png\" alt=\"Cover\" width=\"200\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7e139",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Poppins', sans-serif; font-size: 15px; padding-bottom: 25px\">\n",
    "\n",
    "  <p style=\"line-height: 1.5; text-align: justify\">\n",
    "    <b>Case Facts</b><br>\n",
    "    This lab follows a methodology framework with the objective to develop a proof-of-concept model for credit card fraud detection to address the shortcomings of the existing rule-based system. The team is tasked to directly address the core technical challenge with the data being severely imbalanced. Following Mara's approach, the team will prioritize a high detection rate (recall) and fraud capture rate (FCR).\n",
    "  </p>\n",
    "\n",
    "  <p style=\"line-height: 1.5; text-align: justify\">\n",
    "    <b>Exploratory Data Analysis</b><br>\n",
    "    The team is already provided with the data used by CreditByte. When opening the provided csv file, the dataset contains: transaction id, V1-V28, Amount, and Outcome. V1-V28 are principal components from a PCA operation done to mask PIIs. The 'Outcome' column indicates whether the transaction is fraudulent (1) or not (0). The team will analyze the distributions of these features to identify initial patterns that may lead to additional insights.\n",
    "  </p>\n",
    "\n",
    "  <p style=\"line-height: 1.5; text-align: justify\">\n",
    "    <b>Data Preprocessing</b><br>\n",
    "    In this phase, the team has selected and tested several different sampling methods to preprocess the data and address the severe class imbalance. The sampling methods that will be focused on are: SMOTE, SMOTE + Tomek, ADASYN, ADASYN + Tomek, Random Undersampling, Random Oversampling and Class Weights. These seven sampling methods are tested through various metrics and are tuned in order to maximize effectiveness.\n",
    "  </p>\n",
    "\n",
    "  <p style=\"line-height: 1.5; text-align: justify\">\n",
    "    <b>Modeling & Tuning</b><br>\n",
    "    The Modeling & Tuning phase of the project uses Random Forest classifier with hyperparameter tuning and applying GridSearch on a small parameter grid. This allows the team to focus more on the sampling methods. Through using GridSearch with the various parameters of these sampling methods, the team is able to find the optimal set of parameters that will satisfy the objectives of this project. The objective remains to maximize recall and FCR.\n",
    "  </p>\n",
    "\n",
    "  <p style=\"line-height: 1.5; text-align: justify\">\n",
    "    <b>Insights</b><br>\n",
    "    The main metrics these sampling methods will be tested on are the recall and the FCR achieved by the different sampling methods. Accompanying these are additional metrics which are: Precision, F1, and Net Savings. These performance metrics will help justify the recommended sampling strategy. A mix of business and technical metrics will help solidify our choice of sampling method.\n",
    "  </p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"padding-bottom: 7px;\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55655c93-ca30-4c0f-bb17-856d3b218be6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"background-color: #215880; padding: 10px; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #e6f1fa; font-size: 30px; font-weight: bold; margin: 10px;\">EXPLORATORY DATA ANALYSIS</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5aca50c5-127a-45bb-9b8b-2365c67cda46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T03:40:04.023228Z",
     "iopub.status.busy": "2025-09-29T03:40:04.021261Z",
     "iopub.status.idle": "2025-09-29T03:40:04.308915Z",
     "shell.execute_reply": "2025-09-29T03:40:04.303349Z",
     "shell.execute_reply.started": "2025-09-29T03:40:04.023049Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "value_counts = data[\"outcome\"].value_counts()\n",
    "total = len(data)\n",
    "'''\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "ax = sns.barplot(\n",
    "    x=value_counts.index.map({0:\"No Fraud\", 1:\"Fraud\"}), \n",
    "    y=value_counts.values, \n",
    "    hue=value_counts.index.map({0:\"No Fraud\", 1:\"Fraud\"}),  # assign hue\n",
    "    palette=\"pastel\",\n",
    ")\n",
    "'''\n",
    "\n",
    "# Add percentage labels on top of bars\n",
    "for i, count in enumerate(value_counts.values):\n",
    "    percent = 100 * count / total\n",
    "    ax.text(\n",
    "        i, count + 1000, f\"{percent:.2f}%\", \n",
    "        ha='center', va='bottom', fontsize=12, fontweight='bold'\n",
    "    )\n",
    "\n",
    "\n",
    "#plt.title(\"Value Counts of Outcome\")\n",
    "#plt.ylabel(\"Count\")\n",
    "#plt.xlabel(\"Outcome\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34679d18-70ff-4172-970b-80df6e166222",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://imgur.com/jPCJSxy.png\" alt=\"Boxplots of Numerical Features\" width=\"700\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89facdcb-a699-4bdf-af42-bf44a3ff0bb1",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<p style=\"text-align: center; font-size: 14px; margin-bottom: 30px; margin-left: 100px; font-style: italic;\">\n",
    "    Figure 1. Count of fraudulent and non-fraudulent cases in data set\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c659ac1-c13f-48f0-b966-96fe844c5249",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Poppins', sans-serif; font-size: 15px; padding-bottom: 25px\">\n",
    "<p style=\"line-height: 1.5; text-align: justify\">\n",
    "Figure 1 shows the count of the number of fraudulent and non-fraudulent cases within the data set. According to the bar graph only 0.2% of the data are fradulent cases. Therefore, telling us that there is a high class imbalance. An imbalance data set leads to misleading accuracies,  biased learning, and poor generalization. In other words, it makes it harder for models to identify the patterns of the minority class. Moving forward, the team will explore different methods of countering this imbalance.\n",
    "</p>                                                                                                                                                           \n",
    "</div>\n",
    "\n",
    "<div style=\"padding-bottom: 7px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac6374-c859-47f1-8487-cb0593478778",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "corr_all = data.drop(columns=[\"tid\"]).corr()\n",
    "\n",
    "# Split by outcome\n",
    "corr_outcome1 = data[data[\"outcome\"] == 1].drop(columns=[\"tid\"]).corr()\n",
    "corr_outcome0 = data[data[\"outcome\"] == 0].drop(columns=[\"tid\"]).corr()\n",
    "\n",
    "# Plot\n",
    "#fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "#sns.heatmap(corr_all, annot=False, cmap=\"coolwarm\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Correlation Heatmap (All Data)\", fontsize=14)\n",
    "\n",
    "#sns.heatmap(corr_outcome1, annot=False, cmap=\"coolwarm\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Correlation Heatmap (Outcome = 1)\", fontsize=14)\n",
    "\n",
    "#sns.heatmap(corr_outcome0, annot=False, cmap=\"coolwarm\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Correlation Heatmap (Outcome = 0)\", fontsize=14)\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2becd468-31f4-423f-9262-0298969ff175",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://imgur.com/KvRIeQw.png\" alt=\"Boxplots of Numerical Features\" width=\"1600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295c311d-5214-4f2b-97ee-24b32573bcec",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<p style=\"text-align: center; font-size: 14px; margin-bottom: 30px; margin-left: 20px; font-style: italic;\">\n",
    "    Figures 2-4. Correlation Heatmaps of PCA Components\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76417e28-2d05-46fd-8f31-74730885ec31",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Poppins', sans-serif; font-size: 15px; padding-bottom: 25px\">\n",
    "<p style=\"line-height: 1.5; text-align: justify\">\n",
    "The first correlation Map shows the correlation of all features with one another, while the others show the correlation of each features for fraudulent cases and non-fraudulent cases individually. Looking at the first correlation heatmap, the various PCAs seem to have little to no correlation to one another. However, when isolating the fraud data points, it reveals the multiple PCAs have strong correlations to one another. Therefore, indicating that the iambalance data makes it difficult to identify patterns within the minority class.\n",
    "</p>                                                                                                                                                           \n",
    "</div>\n",
    "\n",
    "<div style=\"padding-bottom: 7px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88cf7b-df91-40c6-8791-46b0e3d0526f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "subset = data[[f\"V{i}\" for i in range(1, 4)] + [\"outcome\"]]\n",
    "\n",
    "# Create pairplot\n",
    "'''\n",
    "sns.pairplot(\n",
    "    data=subset,\n",
    "    vars=[f\"V{i}\" for i in range(1, 4)],\n",
    "    hue=\"outcome\",\n",
    "    palette=\"Set1\",   # you can try \"coolwarm\", \"husl\", etc.\n",
    "    diag_kind=\"kde\",  # kde or hist for diagonal plots\n",
    "    plot_kws={'alpha':0.6, 's':15}  # make points smaller/transparent for readability\n",
    ")\n",
    "'''\n",
    "#plt.suptitle(\"Pairplot of V1–3 Colored by Outcome\", y=1.02)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e4bbdb-065d-43d3-91b9-d498f0d2170d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://imgur.com/kH8HZxH.png\" alt=\"Boxplots of Numerical Features\" width=\"700\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e81e026-85f9-48e3-9e2d-220c7b4c1173",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<p style=\"text-align: center; font-size: 14px; margin-bottom: 30px; margin-left: 20px; font-style: italic;\">\n",
    "    Figure 5. Pairplot of PCA Components V1-3 by Outcome\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e7d12d-9d3f-4fbd-b276-1f7c60d266e1",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Poppins', sans-serif; font-size: 15px; padding-bottom: 25px\">\n",
    "<p style=\"line-height: 1.5; text-align: justify\">\n",
    "Figure 5 presents the relationship between the different principal components, while highlighting the fraudulent and non-fraudulent cases. Based on the pair plots, the fraud data points do not uniformly mix with the non-fraud data points. It forms it’s own clusters and bands, which means that the different principal components are able to identify significant patterns, showing distinct feature interactions. This is promising because it means the features do contain information to distinguish fraud, but the team needs to implement more than a simple linear model to capture it.\n",
    "</p>                                                                                                                                                           \n",
    "</div>\n",
    "\n",
    "<div style=\"padding-bottom: 7px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b8ac1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10,6))\n",
    "#plt.hist(data['Amount'], bins=100, edgecolor='black', alpha=0.5)\n",
    "#plt.xlabel(\"Transaction Amount\")\n",
    "#plt.ylabel(\"Frequency\")\n",
    "#plt.title(\"Distribution of Transaction Amounts\")\n",
    "\n",
    "# Log scale\n",
    "#plt.yscale(\"log\")\n",
    "\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc76a15",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://imgur.com/3i2TSp3.png\" alt=\"Boxplots of Numerical Features\" width=\"700\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e6e34",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<p style=\"text-align: center; font-size: 14px; margin-bottom: 30px; margin-left: 20px; font-style: italic;\">\n",
    "    Figure 6. Histogram of Distribution of Transaction Amounts\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3655a351",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Poppins', sans-serif; font-size: 15px; border-bottom: 2px solid #2a475e; padding-bottom: 15px\">\n",
    "<p style=\"line-height: 1.5; text-align: justify\">\n",
    "Figure 6 illustrates the highly right-skewed distribution of transaction amounts in the dataset. The majority of transactions are concentrated at relatively low amounts, as shown by the dense cluster between 0 and 5000 units, while a long tail extends to larger values. This imbalance demonstrates that models that miss even a few high-value frauds can lead to significant financial losses.\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"border-bottom: 1px solid #b6c6de; box-shadow: 0px 1px 0px #66c0f4; padding-bottom: 7px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c4d76-a3ed-44fd-b87a-50196db57747",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T09:46:56.921774Z",
     "iopub.status.busy": "2025-09-26T09:46:56.921035Z",
     "iopub.status.idle": "2025-09-26T09:46:56.932657Z",
     "shell.execute_reply": "2025-09-26T09:46:56.930457Z",
     "shell.execute_reply.started": "2025-09-26T09:46:56.921709Z"
    }
   },
   "source": [
    "<div style=\"background-color: #215880; padding: 10px; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #e6f1fa; font-size: 30px; font-weight: bold; margin: 10px;\">RESULTS AND DISCUSSION</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e9e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_savings(y_true, y_pred, amounts):\n",
    "    \"\"\"\n",
    "    Compute Net Savings = sum(TP amounts) - sum(FN amounts)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like, true labels\n",
    "    y_pred : array-like, predicted labels\n",
    "    amounts : array-like, transaction amounts aligned with y_true\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : net savings\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    amounts = np.array(amounts)\n",
    "    \n",
    "    # True Positives: fraud predicted as fraud\n",
    "    tp_amount = amounts[(y_true == 1) & (y_pred == 1)].sum()\n",
    "    \n",
    "    # False Negatives: fraud predicted as non-fraud\n",
    "    fn_amount = amounts[(y_true == 1) & (y_pred == 0)].sum()\n",
    "    \n",
    "    return tp_amount - fn_amount\n",
    "\n",
    "def run_sampler_gridsearch(name, pipeline, param_grid, \n",
    "                           X_train, y_train, X_test, y_test, amt_test, \n",
    "                           results, cv=None, random_state=RANDOM_STATE):\n",
    "    \"\"\"\n",
    "    Run GridSearchCV for a given sampler/classifier pipeline,\n",
    "    optimize for Fraud Recall (FCR), evaluate on test set, \n",
    "    and append results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define scorer for fraud recall\n",
    "    fcr_scorer = make_scorer(recall_score, pos_label=1)\n",
    "    \n",
    "    if cv is None:\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "        \n",
    "    # Grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring=fcr_scorer,   # optimize for FCR\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # Predictions on test set\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    precision = precision_score(y_test, y_pred_test)\n",
    "    recall_macro = recall_score(y_test, y_pred_test, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred_test)\n",
    "    fcr = recall_score(y_test, y_pred_test, pos_label=1) * 100\n",
    "    net_sav = net_savings(y_test, y_pred_test, amt_test)\n",
    "    \n",
    "    # Append to results\n",
    "    results.append({\n",
    "        \"Method\": name,\n",
    "        \"Best Params\": best_params,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall (Macro)\": recall_macro,\n",
    "        \"F1\": f1,\n",
    "        \"FCR (%)\": fcr,\n",
    "        \"Net Savings\": net_sav\n",
    "    })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d516b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "amounts = data['Amount'].copy()\n",
    "X = data.drop(columns=['outcome', 'tid'])\n",
    "y = data['outcome']\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test, amt_train_full, amt_test = train_test_split(\n",
    "    X, y, amounts, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val, amt_train, amt_val = train_test_split(\n",
    "    X_train_full, y_train_full, amt_train_full,\n",
    "    test_size=0.25, random_state=RANDOM_STATE, stratify=y_train_full\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3bb0e972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] END .................................................... total time=  25.1s\n",
      "[CV] END .................................................... total time=  25.6s\n",
      "[CV] END .................................................... total time=  26.0s\n",
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n",
      "[CV] END .smote__k_neighbors=3, smote__sampling_strategy=0.2; total time= 3.2min\n",
      "[CV] END .smote__k_neighbors=5, smote__sampling_strategy=0.2; total time= 3.5min\n",
      "[CV] END .smote__k_neighbors=3, smote__sampling_strategy=0.2; total time= 3.6min\n",
      "[CV] END .smote__k_neighbors=5, smote__sampling_strategy=0.2; total time= 3.7min\n",
      "[CV] END .smote__k_neighbors=3, smote__sampling_strategy=0.2; total time= 3.7min\n",
      "[CV] END .smote__k_neighbors=9, smote__sampling_strategy=0.2; total time= 3.9min\n",
      "[CV] END .smote__k_neighbors=9, smote__sampling_strategy=0.2; total time= 3.9min\n",
      "[CV] END .smote__k_neighbors=7, smote__sampling_strategy=0.2; total time= 3.9min\n",
      "[CV] END .smote__k_neighbors=9, smote__sampling_strategy=0.2; total time= 4.0min\n",
      "[CV] END .smote__k_neighbors=7, smote__sampling_strategy=0.2; total time= 4.0min\n",
      "[CV] END .smote__k_neighbors=5, smote__sampling_strategy=0.2; total time= 4.0min\n",
      "[CV] END .smote__k_neighbors=7, smote__sampling_strategy=0.2; total time= 4.0min\n",
      "[CV] END smote__k_neighbors=11, smote__sampling_strategy=0.2; total time= 2.9min\n",
      "[CV] END smote__k_neighbors=13, smote__sampling_strategy=0.2; total time= 2.8min\n",
      "[CV] END smote__k_neighbors=11, smote__sampling_strategy=0.2; total time= 3.0min\n",
      "[CV] END smote__k_neighbors=11, smote__sampling_strategy=0.2; total time= 2.9min\n",
      "[CV] END smote__k_neighbors=13, smote__sampling_strategy=0.2; total time= 2.8min\n",
      "[CV] END smote__k_neighbors=13, smote__sampling_strategy=0.2; total time= 2.7min\n",
      "[CV] END smote__k_neighbors=15, smote__sampling_strategy=0.2; total time= 2.7min\n",
      "[CV] END smote__k_neighbors=15, smote__sampling_strategy=0.2; total time= 2.7min\n",
      "[CV] END smote__k_neighbors=15, smote__sampling_strategy=0.2; total time= 2.7min\n",
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n",
      "[CV] END smote_tomek__k_neighbors=3, smote_tomek__sampling_strategy=0.2; total time= 4.0min\n",
      "[CV] END smote_tomek__k_neighbors=3, smote_tomek__sampling_strategy=0.2; total time= 4.0min\n",
      "[CV] END smote_tomek__k_neighbors=5, smote_tomek__sampling_strategy=0.2; total time= 4.1min\n",
      "[CV] END smote_tomek__k_neighbors=3, smote_tomek__sampling_strategy=0.2; total time= 4.2min\n",
      "[CV] END smote_tomek__k_neighbors=9, smote_tomek__sampling_strategy=0.2; total time= 4.2min\n",
      "[CV] END smote_tomek__k_neighbors=5, smote_tomek__sampling_strategy=0.2; total time= 4.3min\n",
      "[CV] END smote_tomek__k_neighbors=5, smote_tomek__sampling_strategy=0.2; total time= 4.3min\n",
      "[CV] END smote_tomek__k_neighbors=9, smote_tomek__sampling_strategy=0.2; total time= 4.3min\n",
      "[CV] END smote_tomek__k_neighbors=7, smote_tomek__sampling_strategy=0.2; total time= 4.3min\n",
      "[CV] END smote_tomek__k_neighbors=7, smote_tomek__sampling_strategy=0.2; total time= 4.3min\n",
      "[CV] END smote_tomek__k_neighbors=9, smote_tomek__sampling_strategy=0.2; total time= 4.3min\n",
      "[CV] END smote_tomek__k_neighbors=7, smote_tomek__sampling_strategy=0.2; total time= 4.3min\n",
      "[CV] END smote_tomek__k_neighbors=11, smote_tomek__sampling_strategy=0.2; total time= 2.8min\n",
      "[CV] END smote_tomek__k_neighbors=11, smote_tomek__sampling_strategy=0.2; total time= 2.8min\n",
      "[CV] END smote_tomek__k_neighbors=13, smote_tomek__sampling_strategy=0.2; total time= 3.0min\n",
      "[CV] END smote_tomek__k_neighbors=11, smote_tomek__sampling_strategy=0.2; total time= 3.1min\n",
      "[CV] END smote_tomek__k_neighbors=13, smote_tomek__sampling_strategy=0.2; total time= 3.0min\n",
      "[CV] END smote_tomek__k_neighbors=15, smote_tomek__sampling_strategy=0.2; total time= 3.0min\n",
      "[CV] END smote_tomek__k_neighbors=13, smote_tomek__sampling_strategy=0.2; total time= 3.1min\n",
      "[CV] END smote_tomek__k_neighbors=15, smote_tomek__sampling_strategy=0.2; total time= 3.0min\n",
      "[CV] END smote_tomek__k_neighbors=15, smote_tomek__sampling_strategy=0.2; total time= 3.0min\n",
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n",
      "[CV] END adasyn__n_neighbors=3, adasyn__sampling_strategy=0.2; total time= 3.0min\n",
      "[CV] END adasyn__n_neighbors=5, adasyn__sampling_strategy=0.2; total time= 3.1min\n",
      "[CV] END adasyn__n_neighbors=3, adasyn__sampling_strategy=0.2; total time= 3.1min\n",
      "[CV] END adasyn__n_neighbors=3, adasyn__sampling_strategy=0.2; total time= 3.2min\n",
      "[CV] END adasyn__n_neighbors=5, adasyn__sampling_strategy=0.2; total time= 3.3min\n",
      "[CV] END adasyn__n_neighbors=7, adasyn__sampling_strategy=0.2; total time= 3.4min\n",
      "[CV] END adasyn__n_neighbors=9, adasyn__sampling_strategy=0.2; total time= 3.4min\n",
      "[CV] END adasyn__n_neighbors=5, adasyn__sampling_strategy=0.2; total time= 3.5min\n",
      "[CV] END adasyn__n_neighbors=7, adasyn__sampling_strategy=0.2; total time= 3.5min\n",
      "[CV] END adasyn__n_neighbors=9, adasyn__sampling_strategy=0.2; total time= 3.5min\n",
      "[CV] END adasyn__n_neighbors=7, adasyn__sampling_strategy=0.2; total time= 3.6min\n",
      "[CV] END adasyn__n_neighbors=9, adasyn__sampling_strategy=0.2; total time= 3.7min\n",
      "[CV] END adasyn__n_neighbors=11, adasyn__sampling_strategy=0.2; total time= 2.9min\n",
      "[CV] END adasyn__n_neighbors=11, adasyn__sampling_strategy=0.2; total time= 2.9min\n",
      "[CV] END adasyn__n_neighbors=13, adasyn__sampling_strategy=0.2; total time= 2.9min\n",
      "[CV] END adasyn__n_neighbors=11, adasyn__sampling_strategy=0.2; total time= 3.0min\n",
      "[CV] END adasyn__n_neighbors=15, adasyn__sampling_strategy=0.2; total time= 2.7min\n",
      "[CV] END adasyn__n_neighbors=13, adasyn__sampling_strategy=0.2; total time= 2.8min\n",
      "[CV] END adasyn__n_neighbors=13, adasyn__sampling_strategy=0.2; total time= 2.8min\n",
      "[CV] END adasyn__n_neighbors=15, adasyn__sampling_strategy=0.2; total time= 2.7min\n",
      "[CV] END adasyn__n_neighbors=15, adasyn__sampling_strategy=0.2; total time= 2.6min\n",
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n",
      "[CV] END adasyn_tomek__n_neighbors=3, adasyn_tomek__sampling_strategy=0.2; total time= 3.2min\n",
      "[CV] END adasyn_tomek__n_neighbors=3, adasyn_tomek__sampling_strategy=0.2; total time= 3.3min\n",
      "[CV] END adasyn_tomek__n_neighbors=5, adasyn_tomek__sampling_strategy=0.2; total time= 3.4min\n",
      "[CV] END adasyn_tomek__n_neighbors=3, adasyn_tomek__sampling_strategy=0.2; total time= 3.4min\n",
      "[CV] END adasyn_tomek__n_neighbors=5, adasyn_tomek__sampling_strategy=0.2; total time= 3.6min\n",
      "[CV] END adasyn_tomek__n_neighbors=7, adasyn_tomek__sampling_strategy=0.2; total time= 3.6min\n",
      "[CV] END adasyn_tomek__n_neighbors=9, adasyn_tomek__sampling_strategy=0.2; total time= 3.6min\n",
      "[CV] END adasyn_tomek__n_neighbors=7, adasyn_tomek__sampling_strategy=0.2; total time= 3.6min\n",
      "[CV] END adasyn_tomek__n_neighbors=5, adasyn_tomek__sampling_strategy=0.2; total time= 3.6min\n",
      "[CV] END adasyn_tomek__n_neighbors=9, adasyn_tomek__sampling_strategy=0.2; total time= 3.6min\n",
      "[CV] END adasyn_tomek__n_neighbors=7, adasyn_tomek__sampling_strategy=0.2; total time= 3.6min\n",
      "[CV] END adasyn_tomek__n_neighbors=9, adasyn_tomek__sampling_strategy=0.2; total time= 3.6min\n",
      "[CV] END adasyn_tomek__n_neighbors=11, adasyn_tomek__sampling_strategy=0.2; total time= 2.7min\n",
      "[CV] END adasyn_tomek__n_neighbors=11, adasyn_tomek__sampling_strategy=0.2; total time= 2.6min\n",
      "[CV] END adasyn_tomek__n_neighbors=13, adasyn_tomek__sampling_strategy=0.2; total time= 2.6min\n",
      "[CV] END adasyn_tomek__n_neighbors=11, adasyn_tomek__sampling_strategy=0.2; total time= 3.0min\n",
      "[CV] END adasyn_tomek__n_neighbors=13, adasyn_tomek__sampling_strategy=0.2; total time= 2.8min\n",
      "[CV] END adasyn_tomek__n_neighbors=15, adasyn_tomek__sampling_strategy=0.2; total time= 2.8min\n",
      "[CV] END adasyn_tomek__n_neighbors=15, adasyn_tomek__sampling_strategy=0.2; total time= 2.8min\n",
      "[CV] END adasyn_tomek__n_neighbors=13, adasyn_tomek__sampling_strategy=0.2; total time= 2.8min\n",
      "[CV] END adasyn_tomek__n_neighbors=15, adasyn_tomek__sampling_strategy=0.2; total time= 2.8min\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] END .................................................... total time=   0.5s\n",
      "[CV] END .................................................... total time=   0.5s\n",
      "[CV] END .................................................... total time=   0.5s\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] END .................................................... total time=  31.5s\n",
      "[CV] END .................................................... total time=  31.7s\n",
      "[CV] END .................................................... total time=  31.8s\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END clf__class_weight={0: np.float64(0.0), 1: np.float64(100.0)}; total time=   3.1s\n",
      "[CV] END clf__class_weight={0: np.float64(0.0), 1: np.float64(100.0)}; total time=   3.6s\n",
      "[CV] END clf__class_weight={0: np.float64(0.0), 1: np.float64(100.0)}; total time=   3.6s\n",
      "[CV] END clf__class_weight={0: np.float64(100.0), 1: np.float64(0.0)}; total time=   4.3s\n",
      "[CV] END clf__class_weight={0: np.float64(100.0), 1: np.float64(0.0)}; total time=   4.0s\n",
      "[CV] END clf__class_weight={0: np.float64(100.0), 1: np.float64(0.0)}; total time=   4.4s\n",
      "[CV] END clf__class_weight={0: np.float64(25.0), 1: np.float64(75.0)}; total time= 2.0min\n",
      "[CV] END clf__class_weight={0: np.float64(25.0), 1: np.float64(75.0)}; total time= 2.0min\n",
      "[CV] END clf__class_weight={0: np.float64(25.0), 1: np.float64(75.0)}; total time= 2.0min\n",
      "[CV] END clf__class_weight={0: np.float64(50.0), 1: np.float64(50.0)}; total time= 2.0min\n",
      "[CV] END clf__class_weight={0: np.float64(50.0), 1: np.float64(50.0)}; total time= 2.1min\n",
      "[CV] END clf__class_weight={0: np.float64(50.0), 1: np.float64(50.0)}; total time= 2.1min\n",
      "[CV] END clf__class_weight={0: np.float64(75.0), 1: np.float64(25.0)}; total time= 2.1min\n",
      "[CV] END clf__class_weight={0: np.float64(75.0), 1: np.float64(25.0)}; total time= 2.1min\n",
      "[CV] END clf__class_weight={0: np.float64(75.0), 1: np.float64(25.0)}; total time= 2.1min\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Baseline\n",
    "pipeline_rf = Pipeline([\n",
    "    ('clf', RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "param_grid_rf = {}\n",
    "\n",
    "results = run_sampler_gridsearch(\"Random Forest (untuned)\", pipeline_rf, param_grid_rf, \n",
    "                                 X_train, y_train, X_test, y_test, amt_test,\n",
    "                                 results, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "\n",
    "# SMOTE\n",
    "pipeline_smote = Pipeline([\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('clf', RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=500, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_grid_smote = {\n",
    "    'smote__sampling_strategy': [0.2],\n",
    "    'smote__k_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "}\n",
    "\n",
    "results = run_sampler_gridsearch(\"SMOTE\", pipeline_smote, param_grid_smote,\n",
    "                                 X_train, y_train, X_test, y_test, amt_test,\n",
    "                                 results, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "\n",
    "# SMOTE + Tomek\n",
    "pipeline_smote_tomek = Pipeline([\n",
    "    ('smote_tomek', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('tomek', TomekLinks()),\n",
    "    ('clf', RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=500, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_grid_smote_tomek = {\n",
    "    'smote_tomek__sampling_strategy': [0.2],\n",
    "    'smote_tomek__k_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "}\n",
    "\n",
    "results = run_sampler_gridsearch(\"SMOTE + Tomek\", pipeline_smote_tomek, param_grid_smote_tomek,\n",
    "                                 X_train, y_train, X_test, y_test, amt_test,\n",
    "                                 results, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "\n",
    "# ADASYN\n",
    "pipeline_adasyn = Pipeline([\n",
    "    ('adasyn', ADASYN(random_state=RANDOM_STATE)),\n",
    "    ('clf', RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=500, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_grid_adasyn = {\n",
    "    'adasyn__sampling_strategy': [0.2],\n",
    "    'adasyn__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "}\n",
    "\n",
    "results = run_sampler_gridsearch(\"ADASYN\", pipeline_adasyn, param_grid_adasyn,\n",
    "                                 X_train, y_train, X_test, y_test, amt_test,\n",
    "                                 results, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ADASYN + Tomek\n",
    "pipeline_adasyn_tomek = Pipeline([\n",
    "    ('adasyn_tomek', ADASYN(random_state=RANDOM_STATE)),\n",
    "    ('tomek', TomekLinks()),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_estimators=500,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid_adasyn_tomek = {\n",
    "    'adasyn_tomek__sampling_strategy': [0.2],\n",
    "    'adasyn_tomek__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "}\n",
    "\n",
    "results = run_sampler_gridsearch(\"ADASYN + Tomek\", pipeline_adasyn_tomek, param_grid_adasyn_tomek,\n",
    "                                 X_train, y_train, X_test, y_test, amt_test,\n",
    "                                 results, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "\n",
    "# Random Undersampling\n",
    "pipeline_under = Pipeline([\n",
    "    ('under', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_estimators=500,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid_under = {}\n",
    "\n",
    "results = run_sampler_gridsearch(\"Random Undersampling\", pipeline_under, param_grid_under,\n",
    "                                 X_train, y_train, X_test, y_test, amt_test,\n",
    "                                 results, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "\n",
    "# Random Oversampling\n",
    "pipeline_over = Pipeline([\n",
    "    ('over', RandomOverSampler(random_state=RANDOM_STATE)),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_estimators=500,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid_over = {}\n",
    "\n",
    "results = run_sampler_gridsearch(\"Random Oversampling\", pipeline_over,\n",
    "    param_grid_over,\n",
    "    X_train, y_train, X_test, y_test, amt_test,\n",
    "    results, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "\n",
    "# Class weighting\n",
    "weights = np.linspace(0, 100, 5)\n",
    "\n",
    "pipeline_cw = Pipeline([\n",
    "    ('clf', RandomForestClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_estimators=500,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid_cw = {\n",
    "    'clf__class_weight': [{0: w0, 1: 100 - w0} for w0 in weights]\n",
    "}\n",
    "\n",
    "results = run_sampler_gridsearch(\"Class Weights\", pipeline_cw, param_grid_cw,\n",
    "                                 X_train, y_train, X_test, y_test, amt_test,\n",
    "                                 results, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fa762b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall (Macro)</th>\n",
       "      <th>F1</th>\n",
       "      <th>FCR (%)</th>\n",
       "      <th>Net Savings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest (untuned)</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.851508</td>\n",
       "      <td>0.796460</td>\n",
       "      <td>70.3125</td>\n",
       "      <td>-670.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>{'smote__k_neighbors': 13, 'smote__sampling_st...</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>0.906101</td>\n",
       "      <td>0.818898</td>\n",
       "      <td>81.2500</td>\n",
       "      <td>688.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SMOTE + Tomek</td>\n",
       "      <td>{'smote_tomek__k_neighbors': 11, 'smote_tomek_...</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>0.906101</td>\n",
       "      <td>0.818898</td>\n",
       "      <td>81.2500</td>\n",
       "      <td>688.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADASYN</td>\n",
       "      <td>{'adasyn__n_neighbors': 15, 'adasyn__sampling_...</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.906088</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>81.2500</td>\n",
       "      <td>688.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADASYN + Tomek</td>\n",
       "      <td>{'adasyn_tomek__n_neighbors': 13, 'adasyn_tome...</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.906088</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>81.2500</td>\n",
       "      <td>688.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Undersampling</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.057436</td>\n",
       "      <td>0.925068</td>\n",
       "      <td>0.107796</td>\n",
       "      <td>87.5000</td>\n",
       "      <td>4771.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Oversampling</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.874946</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>75.0000</td>\n",
       "      <td>-430.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Class Weights</td>\n",
       "      <td>{'clf__class_weight': {0: 0.0, 1: 100.0}}</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>7177.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Method                                        Best Params  \\\n",
       "0  Random Forest (untuned)                                                 {}   \n",
       "1                    SMOTE  {'smote__k_neighbors': 13, 'smote__sampling_st...   \n",
       "2            SMOTE + Tomek  {'smote_tomek__k_neighbors': 11, 'smote_tomek_...   \n",
       "3                   ADASYN  {'adasyn__n_neighbors': 15, 'adasyn__sampling_...   \n",
       "4           ADASYN + Tomek  {'adasyn_tomek__n_neighbors': 13, 'adasyn_tome...   \n",
       "5     Random Undersampling                                                 {}   \n",
       "6      Random Oversampling                                                 {}   \n",
       "7            Class Weights          {'clf__class_weight': {0: 0.0, 1: 100.0}}   \n",
       "\n",
       "   Precision  Recall (Macro)        F1   FCR (%)  Net Savings  \n",
       "0   0.918367        0.851508  0.796460   70.3125      -670.45  \n",
       "1   0.825397        0.906101  0.818898   81.2500       688.95  \n",
       "2   0.825397        0.906101  0.818898   81.2500       688.95  \n",
       "3   0.812500        0.906088  0.812500   81.2500       688.95  \n",
       "4   0.812500        0.906088  0.812500   81.2500       688.95  \n",
       "5   0.057436        0.925068  0.107796   87.5000      4771.63  \n",
       "6   0.923077        0.874946  0.827586   75.0000      -430.41  \n",
       "7   0.001729        0.500000  0.003451  100.0000      7177.73  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_final = pd.DataFrame(results)\n",
    "results_df_final.to_csv('results_amount.csv', index=False)\n",
    "results_df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ee761",
   "metadata": {},
   "source": [
    " <center>\n",
    "    <h2>Summary of Results</h2>\n",
    "</center>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Machine Learning Method | Precision | Recall (Macro) | F1 Score | FCR (%) | Net Savings |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| Random Forest (untuned) | 0.918 | 0.852 | 0.796 | 70.31 | -670.45 |\n",
    "| SMOTE | 0.825 | 0.906 | 0.819 | 81.25 | 688.95 |\n",
    "| SMOTE + Tomek | 0.825 | 0.906 | 0.819 | 81.25 | 688.95 |\n",
    "| ADASYN | 0.813 | 0.906 | 0.813 | 81.25 | 688.95 |\n",
    "| ADASYN + Tomek | 0.813 | 0.906 | 0.813 | 81.25 | 688.95 |\n",
    "| Random Undersampling | 0.057 | 0.925 | 0.108 | 87.50 | 4771.63 |\n",
    "| Random Oversampling | 0.923 | 0.875 | 0.828 | 75.00 | -430.41 |\n",
    "| Class Weights | 0.002 | 0.500 | 0.003 | 100.0 | 7177.73 |\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b629fc",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Poppins', sans-serif; font-size: 15px; border-bottom: 2px solid #2a475e; padding-bottom: 15px\">\n",
    "<p style=\"line-height: 1.5; text-align: justify\">\n",
    "To evaluate different strategies for handling the imbalance of fraudulent cases in the data set, a Random Forest classifier was trained with various resampling and weighting methods. The performance was measured using both machine learning metrics: precision, recall, and F1 score, and CreditByte’s domain-specific metrics: detection rate and fraud capture rate (FCR). Detection rate is equivalent to recall, which indicates the proportion of fraud cases correctly identified. FCR calculates the percentage of all fraud transactions flagged by the model. Additionally, net savings was computed to quantify business impact, representing the financial benefit of prevented fraud minus the losses from missed fraud.\n",
    "</p>\n",
    "\n",
    "<p style=\"line-height: 1.5; text-align: justify\">\n",
    "The baseline Random Forest achieved strong precision of 0.91 and recall of 0.85, but still produced a negative net savings. This would suggest that while the model caught most fraudulent cases, the few high-value frauds it missed outweighed the savings from correctly detected ones. Applying oversampling methods such as SMOTE and ADASYN traded a slight decrease in precision for an improvent in all other metrics, with consistent positive net savings.\n",
    "</p>\n",
    "\n",
    "<p style=\"line-height: 1.5; text-align: justify\">\n",
    "Random undersampling produced the highest detection rate with recall reaching 0.93 and an FCR of 87.5%. From a financial perspective, this method returned 7 times the net savings of SMOTE and ADASYN. However, the significant savings came at the cost of a steep decline in precision of 0.06. This may indicate that the model underfits the majority class, while over-predicting fraud; The model increasingly flags legitimate transactions as fraudulent as it hasn't learned enough what a non-fraudulent transaction is.\n",
    "</p>\n",
    "\n",
    "<p style=\"line-height: 1.5; text-align: justify\">\n",
    "Random oversampling produced a recall of 0.87 and an FCR of 75.0%, which were improvements over the baseline Random Forest, but fell short in comparison to the balanced results of the synthetic oversampling methods. Precision remained relatively high at 0.92, but the financial outcome was negative with net savings of –430.41. Even if oversampling preserved the information from the majority class and helped the model avoid a severe dip in precision, it also introduced the risk of overfitting to repeated minority samples. Therefore, the model performed better at distinguishing legitimate transactions from fraud, yet failed to capture enough fraudulent cases to be of any positive business value.\n",
    "</p>\n",
    "\n",
    "<p style=\"line-height: 1.5; text-align: justify\">\n",
    "The class weighting showed the most extreme trade-off by forcing the model to prioritize fraud detection at all costs, hence the FCR of 100%, but precision, recall, and F1 score collapsed. Although this resulted in the largest calculated savings, such a model would overwhelm CreditByte with false positives.\n",
    "</p>\n",
    "\n",
    "<p style=\"line-height: 1.5; text-align: justify\">\n",
    "In summary, oversampling strategies such as SMOTE and ADASYN show the best balance between detection and precision, and has financial viability from the positive net savings. Meanwhile, undersampling and class weighting maximize fraud capture and financial returns but is inefficient at correctly tagging fraud cases.\n",
    "</p>\n",
    "\n",
    "<p style=\"line-height: 1.5; text-align: justify\">\n",
    "<b>The team recommends SMOTE</b> as the method for handling the imbalanced data set due to consistent results in detection and fraud capture, while generating positive net savings, making it the most defensible option both a technical and business standpoint. Over time, hybrid approaches such as SMOTE with Tomek Links may further improve performance by balancing fraud detection with reduced false positives.\n",
    "</p>\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"border-bottom: 1px solid #b6c6de; box-shadow: 0px 1px 0px #66c0f4; padding-bottom: 7px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1767bfb8",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #215880; padding: 10px; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #e6f1fa; font-size: 30px; font-weight: bold; margin: 10px;\">CONCLUSION AND RECOMMENDATIONS</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e5360",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Poppins', sans-serif; font-size: 15px; border-bottom: 2px solid #2a475e; padding-bottom: 15px\">\n",
    "<p style=\"line-height: 1.5; text-align: justify\">\n",
    "In conclusion, the lab demonstrates the importance of handling class imbalance in fraud detection to improve technical performance and business profitability. Among the methods tested, undersampling and class weighting achieved the highest fraud capture but was outweighed by the drawbacks from low precision. SMOTE and other oversampling methods had the best balance and yielded consistent improvements in recall and FCR while maintaining financial viability through the positive net savings. These results demonstrate that SMOTE is the most appropriate technique for this dataset, and future works with hybrid methods may enhance model effectiveness.\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"border-bottom: 1px solid #b6c6de; box-shadow: 0px 1px 0px #66c0f4; padding-bottom: 7px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2e089d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #215880; padding: 10px; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #e6f1fa; font-size: 30px; font-weight: bold; margin: 10px;\">SUPPLEMENTARY MATERIALS</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed3578-d120-4f60-966d-51a6cbb37f10",
   "metadata": {},
   "source": [
    "<div style=\"padding: 0px; border-radius: 5px;\">\n",
    "    <h5 style=\"color: #215880; font-size: 20px; font-weight: bold; margin: 10px;\">Applied pre-processing by scaling; Did not improve performance</h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "89304e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOKY_MAX_CPU_COUNT set to 12\n"
     ]
    }
   ],
   "source": [
    "# needed for cores in macbook\n",
    "import os, multiprocessing\n",
    "\n",
    "n_logical = multiprocessing.cpu_count()\n",
    "\n",
    "n_logical = os.cpu_count()   # same as multiprocessing.cpu_count()\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = str(n_logical)\n",
    "print(f\"LOKY_MAX_CPU_COUNT set to {n_logical}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5fe4877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep Amount separately for net_savings\n",
    "amounts = data['Amount'].copy()\n",
    "\n",
    "# Features: keep Amount inside X (so model can use it)\n",
    "X = data.drop(columns=['outcome', 'tid'])\n",
    "y = data['outcome']\n",
    "\n",
    "# Split, carrying amounts separately for evaluation\n",
    "X_train_full, X_test, y_train_full, y_test, amt_train_full, amt_test = train_test_split(\n",
    "    X, y, amounts, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val, amt_train, amt_val = train_test_split(\n",
    "    X_train_full, y_train_full, amt_train_full,\n",
    "    test_size=0.25, random_state=RANDOM_STATE, stratify=y_train_full\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9d0ae052-681f-446b-854c-c830e54f24c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] END .................................................... total time=  25.0s\n",
      "[CV] END .................................................... total time=  25.5s\n",
      "[CV] END .................................................... total time=  25.8s\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Baseline\n",
    "pipeline_rf = Pipeline([\n",
    "    ('clf', RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "param_grid_rf = {}\n",
    "\n",
    "results = run_sampler_gridsearch(\"Random Forest (untuned)\", pipeline_rf, param_grid_rf, \n",
    "                                 X_train, y_train, X_test, y_test, amt_test,\n",
    "                                 results, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "\n",
    "# SMOTE\n",
    "pipeline_smote = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('clf', RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=500, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_grid_smote = {\n",
    "    'smote__sampling_strategy': [0.2],\n",
    "    'smote__k_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "}\n",
    "\n",
    "results = run_sampler_gridsearch(\"SMOTE\", pipeline_smote, param_grid_smote,\n",
    "                                 X_train, y_train, X_test, y_test, amt_test,\n",
    "                                 results, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "\n",
    "# SMOTE + Tomek\n",
    "pipeline_smote_tomek = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote_tomek', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('tomek', TomekLinks()),\n",
    "    ('clf', RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=500, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_grid_smote_tomek = {\n",
    "    'smote_tomek__sampling_strategy': [0.2],\n",
    "    'smote_tomek__k_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "}\n",
    "\n",
    "results = run_sampler_gridsearch(\"SMOTE + Tomek\", pipeline_smote_tomek, param_grid_smote_tomek,\n",
    "                                 X_train, y_train, X_test, y_test, amt_test,\n",
    "                                 results, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "\n",
    "# ADASYN\n",
    "pipeline_adasyn = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('adasyn', ADASYN(random_state=RANDOM_STATE)),\n",
    "    ('clf', RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=500, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_grid_adasyn = {\n",
    "    'adasyn__sampling_strategy': [0.2],\n",
    "    'adasyn__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "}\n",
    "\n",
    "results = run_sampler_gridsearch(\"ADASYN\", pipeline_adasyn, param_grid_adasyn,\n",
    "                                 X_train, y_train, X_test, y_test, amt_test,\n",
    "                                 results, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ADASYN + Tomek\n",
    "pipeline_adasyn_tomek = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('adasyn_tomek', ADASYN(random_state=RANDOM_STATE)),\n",
    "    ('tomek', TomekLinks()),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_estimators=500,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid_adasyn_tomek = {\n",
    "    'adasyn_tomek__sampling_strategy': [0.2],\n",
    "    'adasyn_tomek__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "}\n",
    "\n",
    "results = run_sampler_gridsearch(\"ADASYN + Tomek\", pipeline_adasyn_tomek, param_grid_adasyn_tomek,\n",
    "                                 X_train, y_train, X_test, y_test, amt_test,\n",
    "                                 results, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "\n",
    "# Random Undersampling\n",
    "pipeline_under = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('under', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_estimators=500,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid_under = {}\n",
    "\n",
    "results = run_sampler_gridsearch(\"Random Undersampling\", pipeline_under, param_grid_under,\n",
    "                                 X_train, y_train, X_test, y_test, amt_test,\n",
    "                                 results, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "\n",
    "# Random Oversampling\n",
    "pipeline_over = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('over', RandomOverSampler(random_state=RANDOM_STATE)),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_estimators=500,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid_over = {}\n",
    "\n",
    "results = run_sampler_gridsearch(\"Random Oversampling\", pipeline_over,\n",
    "    param_grid_over,\n",
    "    X_train, y_train, X_test, y_test, amt_test,\n",
    "    results, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "\n",
    "# Class weighting\n",
    "weights = np.linspace(0, 100, 5)\n",
    "\n",
    "pipeline_cw = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_estimators=500,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid_cw = {\n",
    "    'clf__class_weight': [{0: w0, 1: 100 - w0} for w0 in weights]\n",
    "}\n",
    "\n",
    "results = run_sampler_gridsearch(\"Class Weights\", pipeline_cw, param_grid_cw,\n",
    "                                 X_train, y_train, X_test, y_test, amt_test,\n",
    "                                 results, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "855bfd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall (Macro)</th>\n",
       "      <th>F1</th>\n",
       "      <th>FCR (%)</th>\n",
       "      <th>Net Savings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest (untuned)</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.851508</td>\n",
       "      <td>0.796460</td>\n",
       "      <td>70.3125</td>\n",
       "      <td>-670.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>{'smote__k_neighbors': 13, 'smote__sampling_st...</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.890436</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>78.1250</td>\n",
       "      <td>-423.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SMOTE + Tomek</td>\n",
       "      <td>{'smote_tomek__k_neighbors': 13, 'smote_tomek_...</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.890436</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>78.1250</td>\n",
       "      <td>-423.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADASYN</td>\n",
       "      <td>{'adasyn__n_neighbors': 13, 'adasyn__sampling_...</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.890436</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>78.1250</td>\n",
       "      <td>-423.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADASYN + Tomek</td>\n",
       "      <td>{'adasyn_tomek__n_neighbors': 13, 'adasyn_tome...</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.890436</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>78.1250</td>\n",
       "      <td>-423.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Undersampling</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.057436</td>\n",
       "      <td>0.925068</td>\n",
       "      <td>0.107796</td>\n",
       "      <td>87.5000</td>\n",
       "      <td>4771.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Oversampling</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.874946</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>75.0000</td>\n",
       "      <td>-430.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Class Weights</td>\n",
       "      <td>{'clf__class_weight': {0: 0.0, 1: 100.0}}</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>7177.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Method                                        Best Params  \\\n",
       "0  Random Forest (untuned)                                                 {}   \n",
       "1                    SMOTE  {'smote__k_neighbors': 13, 'smote__sampling_st...   \n",
       "2            SMOTE + Tomek  {'smote_tomek__k_neighbors': 13, 'smote_tomek_...   \n",
       "3                   ADASYN  {'adasyn__n_neighbors': 13, 'adasyn__sampling_...   \n",
       "4           ADASYN + Tomek  {'adasyn_tomek__n_neighbors': 13, 'adasyn_tome...   \n",
       "5     Random Undersampling                                                 {}   \n",
       "6      Random Oversampling                                                 {}   \n",
       "7            Class Weights          {'clf__class_weight': {0: 0.0, 1: 100.0}}   \n",
       "\n",
       "   Precision  Recall (Macro)        F1   FCR (%)  Net Savings  \n",
       "0   0.918367        0.851508  0.796460   70.3125      -670.45  \n",
       "1   0.781250        0.890436  0.781250   78.1250      -423.97  \n",
       "2   0.781250        0.890436  0.781250   78.1250      -423.97  \n",
       "3   0.781250        0.890436  0.781250   78.1250      -423.97  \n",
       "4   0.781250        0.890436  0.781250   78.1250      -423.97  \n",
       "5   0.057436        0.925068  0.107796   87.5000      4771.63  \n",
       "6   0.923077        0.874946  0.827586   75.0000      -430.41  \n",
       "7   0.001729        0.500000  0.003451  100.0000      7177.73  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df_amount_scaled = results_df\n",
    "results_df_amount_scaled.to_csv('results_amount_scaled.csv', index=False)\n",
    "results_df_amount_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d206d-4652-4e85-ab1c-a6d944f85d80",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #215880; padding: 10px; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #e6f1fa; font-size: 30px; font-weight: bold; margin: 10px;\">REFERENCES</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a18d799",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
